{# Set variables #}

{% set flink_version = '1.20.0' %}

{# scala_version is only used when downloading example artifacts.
   We do not include flink-scala in this distribution.  If you want
   to use scala, include it in your application dependencies. #}
{% set scala_version = '2.12' %}

{# python_version is hardcoded here to know the directory
   from which to copy pip installed dependecies to the production image
   This does not control the version of python installed.
   You must update this if you upgrade the version of python that is installed. #}
{% set python_version = '3.9' %}

{# Elastic Common Schema logging version. Used for structured logging. #}
{% set ecs_version = '1.5.0' %}

{% set flink_home = '/opt/flink' %}

{# End variables #}

#
# This Dockerfile installs Apache Flink using python pip.
#
# We use pip instead of installing from a flink distribution tarball
# because doing so also installs required transitive python dependencies
# of pyflink.  pyflink includes the .jar files that usually come with a
# flink distribution, so using it to run JVM based apps should be fine.
#


# == Beginning of build stage ==

# NOTE: build stage does not consoladate RUN commands to take advantage
#       of more cached RUN layers. production image stage below does consolodate
#       RUN commands in order to have fewer layers there.

FROM {{ registry }}/bullseye:latest as build
USER root

RUN {{ "curl gpg gpg-agent dirmngr coreutils ca-certificates build-essential python3 python3-setuptools python3-pip" | apt_install }}
WORKDIR /tmp

RUN pip install apache-flink=={{ flink_version }}

# Symlink the installed pyflink module directory to {{ flink_home }}.
# This will give us a consistent and easier to find path for FLINK_HOME,
# independent of python version changes.
RUN ln -sv $(python3 -c 'import pyflink, os; print(os.path.dirname(pyflink.__file__))') {{ flink_home }}

# Remove flink scala deps -- we run scala free!
# If you want to use scala in your Flink app, make it part of your application jar's included dependencies.
RUN rm -fv {{ flink_home }}/lib/flink-scala*


# Download and verify extra .jars we want in our base Flink image.
COPY maven-download.sh /tmp/maven-download.sh

# ecs-logging-core -> lib/
RUN /tmp/maven-download.sh co.elastic.logging ecs-logging-core {{ ecs_version }} {{ flink_home }}/lib

# log4j-ecs-layout -> lib/
RUN /tmp/maven-download.sh co.elastic.logging log4j2-ecs-layout {{ ecs_version }} {{ flink_home }}/lib

# flink-s3-fs-presto -> opt/
# This is used for Flink checkpointing.
# Since install into opt/, as the Flink tarball distribution would, users of this image must
# install this into plugins/ as described at
# https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/filesystems/plugins/#file-systems
RUN /tmp/maven-download.sh org.apache.flink flink-s3-fs-presto {{ flink_version }} {{ flink_home }}/opt

# pyflink doesn't include some of the java based examples, but we want them for easier testing!
# Download their jars and put them in the right place.
# Move the python examples into examples/python
RUN mkdir /tmp/python-examples && mv {{ flink_home }}/examples/* /tmp/python-examples && mv /tmp/python-examples {{ flink_home }}/examples/python
# Download the examples we want to include from maven
RUN mkdir {{ flink_home }}/examples/streaming {{ flink_home }}/examples/table {{ flink_home }}/examples/batch
RUN /tmp/maven-download.sh org.apache.flink flink-examples-streaming {{ flink_version }} {{ flink_home }}/examples/streaming
RUN /tmp/maven-download.sh org.apache.flink flink-examples-table_{{ scala_version }} {{ flink_version }} {{ flink_home }}/examples/table
RUN /tmp/maven-download.sh org.apache.flink flink-examples-batch {{ flink_version }} {{ flink_home }}/examples/batch

# Copy any extra custom examples WMF adds into examples/wikimedia
# Our python/table_datagen.py example runs forever,
# which makes it more useful for testing deployment of long lived streaming flink apps.
COPY --chown=root  examples/wikimedia {{ flink_home }}/examples/wikimedia

# == End of build stage ==


# == Start of production image creation ==

FROM {{ "openjdk-11-jre" | image_tag }}

USER root

# Copy the pip installed packages from the build images, including pyflink.
COPY --from=build /usr/local/lib/python{{ python_version }} /usr/local/lib/python{{ python_version }}

# RUN:
# - Install debian packages.
#
# - Create the flink user.
#
# - Symlink the installed pyflink module directory to {{ flink_home }} in the production image.
RUN {{ "bash krb5-user libnss3 procps net-tools libsnappy1v5 gettext-base libjemalloc-dev python3 python3-setuptools python-is-python3 wmf-certificates" | apt_install }} && \
    {{ "flink" | add_user }} && \
    ln -sv $(python3 -c 'import pyflink, os; print(os.path.dirname(pyflink.__file__))') {{ flink_home }} && \
    chown flink:flink {{ flink_home }}/conf {{ flink_home }}/plugins {{ flink_home }}/log

# ENV:
#
# - Set FLINK_HOME.
#
# - Include $FLINK_HOME/bin in PATH.  This is needed
#   because the kubernetes operator will attempt to run commands like
#   'kubernetes-jobmanager.sh kubernetes-application', and
#   'kubernetes-jobmanager.sh' is in Flink's bin/ directory.
#
#   NOTE: docker-entrypoint.sh will enable jemalloc by default.
#   Set DISABLE_JEMALLOC=true to use default (glibc) malloc instead.
#
ENV FLINK_HOME={{ flink_home }} \
    PATH={{ flink_home }}/bin:$PATH

WORKDIR $FLINK_HOME

# Copy our custom entrypoint script to this image.
# Flink kubernetes native will run this by default.
# (This is overridable by setting kubernetes.entry.path in flink-conf.yaml
#  but the default is fine.  See comment in docker-entrypoint.sh for more info.)
COPY docker-entrypoint.sh /docker-entrypoint.sh

USER {{ "flink" | uid }}

# NOTE: flink native kubernetes will always run this image with a command constructed using the
# 'kubernetes.entry.path' config, which defaults to /docker-entrypoint.sh.
# We set it here anyway to avoid confusion.
ENTRYPOINT ["/docker-entrypoint.sh"]

# == End of image creation ==
