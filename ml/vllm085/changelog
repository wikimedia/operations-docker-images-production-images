amd-vllm085 (gfx90arocm6.3.0pytorch2.10.0flash-attn2.7.4vllm0.8.5-1) wikimedia; urgency=high

  * Update torch from v2.8.0.dev20250508+rocm6.3 to v2.10.0.dev20250926+rocm6.3

 -- Kevin Bazira <kbazira@wikimedia.org>  Mon, 26 Jan 2026 13:16:10 +0300

amd-vllm085 (gfx90arocm6.3.0pytorch2.8.0flash-attn2.7.4vllm0.8.5-1) wikimedia; urgency=high

  * Initial release with ROCm 6.3.0, PyTorch 2.8.0, CK FlashAttention 2.7.4, and vLLM 0.8.5 targeting MI200 (gfx90a) AMD GPUs.

 -- Kevin Bazira <kbazira@wikimedia.org>  Fri, 16 May 2025 10:35:00 +0300
